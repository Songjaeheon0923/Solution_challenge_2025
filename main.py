import os
import faiss
import numpy as np
import re
import json
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from typing import List
from google.generativeai import configure, GenerativeModel

# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("GEMINI_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# âœ… Gemini API ì„¤ì •
configure(api_key=api_key)
gemini_model = GenerativeModel("models/gemini-1.5-flash")

# âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# âœ… ì„¤ì •
INDEX_ROOT = "embeddings"
CATEGORIES = ["contents", "historical", "preparation"]

# âœ… ì§ˆë¬¸ ì„ë² ë”©
def embed_query(text: str) -> np.ndarray:
    embedding = model.encode([text])[0]
    return np.array([embedding], dtype=np.float32)

# âœ… ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
def search_similar_docs(query: str, category: str, top_k: int = 3) -> List[str]:
    index_path = os.path.join(INDEX_ROOT, f"{category}.index")
    path_txt = os.path.join(INDEX_ROOT, f"{category}_paths.txt")

    if not os.path.exists(index_path) or not os.path.exists(path_txt):
        raise FileNotFoundError(f"{category} ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")

    index = faiss.read_index(index_path)
    with open(path_txt, "r", encoding="utf-8") as f:
        id_map = [line.strip() for line in f.readlines()]

    query_vec = embed_query(query)
    distances, indices = index.search(query_vec, top_k)

    results = []
    for idx in indices[0]:
        if idx < len(id_map):
            with open(id_map[idx], "r", encoding="utf-8") as f:
                results.append(f.read().strip())

    return results

# âœ… ì§ˆë¬¸ ë¶„ë¥˜
def classify_question_with_gemini(question: str) -> str:
    prompt = (
        "ë‹¤ìŒ ë¬¸ì¥ì´ ì—¬í–‰ì— ëŒ€í•œ ì–´ë–¤ ìœ í˜•ì¸ì§€ ë¶„ë¥˜í•´ ì£¼ì„¸ìš”. "
        "ë§Œì•½, ì§ˆë¬¸ì— ë¬¸í™”, ì—­ì‚¬ì™€ ê°™ì€ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ë©´ historicalë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. "
        "ë§Œì•½, ì§ˆë¬¸ì— ìŒì‹, í•  ê²ƒ, ì¦ê¸¸ê±°ë¦¬, ëª…ì†Œ, í’ê²½ê³¼ ê°™ì€ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ë©´ contentsë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. "
        "ë§Œì•½, ì§ˆë¬¸ì— ì¤€ë¹„í•  ê²ƒ, í•„ìš”í•œ ê²ƒ, ì¤€ë¹„ë¬¼ê³¼ ê°™ì€ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ë©´ preparationë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. "
        "ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”: contents, historical, preparation\n\n"
        f"ë¬¸ì¥: {question}"
    )
    response = gemini_model.generate_content(prompt)
    category = response.text.strip().lower()

    if category not in CATEGORIES:
        raise ValueError(f"ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬ ë°˜í™˜ë¨: {category}")
    return category

# âœ… ë‹µë³€ ìƒì„±
def generate_answer_with_gemini(question: str, docs: List[str]) -> str:
    context = "\n---\n".join(docs)
    full_prompt = (
        "ë‹¹ì‹ ì€ ì—¬í–‰ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ìœ ìš©í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"

        f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{question}\n\n"
        f"[ì°¸ê³  ë¬¸ì„œ]\n{context}"
    )
    response = gemini_model.generate_content(full_prompt)
    return response.text.strip()

# âœ… ìš”ì•½ ìƒì„± (ìˆ˜ì • ê¸ˆì§€)
def generate_summary_with_gemini(answer: str) -> str:
    prompt = (
        "ë‹¹ì‹ ì€ ì—¬í–‰ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì„œ 1~3ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”. "
        "ì‘ë‹µì€ ë§ˆí¬ë‹¤ìš´ ì—†ì´ ì¼ë°˜ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ê³ , í•´ë‹¹ ì§€ì—­ì˜ íŠ¹ì„±ì„ ì˜ˆì‹œì™€ í•¨ê»˜ í¬í•¨í•´ì£¼ì„¸ìš”.\n\n"
        f"ë‚´ìš©: {answer}"
    )
    response = gemini_model.generate_content(prompt)
    return response.text.strip()

def clean_markdown(text: str) -> str:
    return text.replace("*", "").strip()


import json
import re

# ğŸ”§ ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ëŸ­(```json ... ```)ë§Œ ì œê±°
def clean_code_block_json(text: str) -> str:
    return re.sub(r"^```json\s*|\s*```$", "", text.strip(), flags=re.IGNORECASE | re.DOTALL)

# ğŸ”§ *ë§Œ ì œê±° (historicalìš©)
def clean_markdown(text: str) -> str:
    return text.replace("*", "").strip()

# âœ… message êµ¬ì¡°í™” í•¨ìˆ˜
def format_message(category: str, answer: str) -> dict | str:
    if category == "historical":
        prompt = (
            "ë‹¤ìŒ ë‚´ìš©ì„ ì‹œëŒ€ë³„ë¡œ ì •ë¦¬í•´ì„œ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜. ê° ì‹œëŒ€ëŠ” í•´ë‹¹ ì§€ì—­ì—ì„œë§Œ ì‚¬ìš©í•˜ëŠ” ê³ ìœ  ì‹œëŒ€ëª…ì´ë©´ ì¢‹ì•„. ê° ì‹œëŒ€ëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ í‘œí˜„í•´ì•¼ í•´:\n\n"
            "{\n"
            "  \"[ì‹œëŒ€ ì´ë¦„]\": {\n"
            "    \"ì‹œëŒ€ ì„¤ëª…\": \"í•´ë‹¹ ì§€ì—­ì˜ ë‹¹ëŒ€ ì—­ì‚¬ì  ë°°ê²½ì— ëŒ€í•œ 2~3ë¬¸ì¥ì˜ ì„¤ëª…\",\n"
            "    \"ì£¼ìš” ì‚¬ê±´\": [\n"
            "      {\"ì´ë¦„\": \"ì‚¬ê±´ëª…\", \"ì„¤ëª…\": \"ê·¸ ì‚¬ê±´ì´ ì™œ ì¤‘ìš”í•œì§€ì— ëŒ€í•œ ì„¤ëª…\"},\n"
            "      ... (ìµœì†Œ 2ê°œ)\n"
            "    ]\n"
            "  },\n"
            "  ...\n"
            "}\n\n"
            "ì¡°ê±´:\n"
            "- ë§ˆí¬ë‹¤ìš´ ì—†ì´ JSONë§Œ ì¶œë ¥\n"
            "- ê° ì‹œëŒ€ë³„ë¡œ ë°˜ë“œì‹œ 'ì‹œëŒ€ ì„¤ëª…'ê³¼ 2ê°œ ì´ìƒì˜ 'ì£¼ìš” ì‚¬ê±´' í¬í•¨\n"
            "- ì‚¬ê±´ ì´ë¦„ì€ ê°„ê²°í•˜ê²Œ, ì„¤ëª…ì€ 2ë¬¸ì¥ ì´ìƒ\n\n"
            f"ë‚´ìš©:\n{answer}"
        )
        response = gemini_model.generate_content(prompt)
        try:
            cleaned = clean_code_block_json(response.text)
            return json.loads(cleaned)
        except json.JSONDecodeError:
            print("âŒ JSON íŒŒì‹± ì˜¤ë¥˜ (historical):\n", response.text)
            raise



    elif category == "contents":
        prompt = (
            "ë‹¤ìŒ ë‚´ìš©ì„ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì¤˜. ê° í•­ëª©ì€ ìµœì†Œ 2ê°œ ì´ìƒ í¬í•¨í•˜ê³ , ë§ˆí¬ë‹¤ìš´ ì—†ì´ JSONë§Œ ì¶œë ¥í•´. "
            "ê° í•­ëª©ì€ nameê³¼ information í•„ë“œë¥¼ í¬í•¨í•˜ê³  imageurlì€ ì œì™¸í•´. "
            "**informationì€ ë‘ ë¬¸ì¥ ì´ìƒìœ¼ë¡œ êµ¬ì„±ëœ ìƒì„¸ ì„¤ëª…ì´ì–´ì•¼ í•´.**"
            "í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì•„:\n"
            "{\n"
            "  \"Place\": [ {\"name\": \"...\", \"information\": \"...\"}, ... ],\n"
            "  \"F&B\": [ {\"name\": \"...\", \"information\": \"...\"}, ... ],\n"
            "  \"Activity\": [ {\"name\": \"...\", \"information\": \"...\"}, ... ]\n"
            "}\n"
            f"\në‚´ìš©:\n{answer}"
        )
        response = gemini_model.generate_content(prompt)
        try:
            cleaned = clean_code_block_json(response.text)
            return json.loads(cleaned)
        except json.JSONDecodeError:
            print("âŒ JSON íŒŒì‹± ì˜¤ë¥˜ (contents):\n", response.text)
            raise

    elif category == "preparation":
        prompt = (
            "ë‹¤ìŒ ë‚´ìš©ì„ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì¤˜. Clothesì™€ ETC í•­ëª© ê°ê° ìµœì†Œ 2ê°œ ì´ìƒ í¬í•¨í•˜ê³ , "
            "ê° í•­ëª©ì€ nameê³¼ information í•„ë“œë¥¼ í¬í•¨í•´. imageurlì€ ì œì™¸í•˜ê³  ë§ˆí¬ë‹¤ìš´ ì—†ì´ JSONë§Œ ì¶œë ¥í•´."
            "**informationì€ ë‘ ë¬¸ì¥ ì´ìƒìœ¼ë¡œ êµ¬ì„±ëœ ìƒì„¸ ì„¤ëª…ì´ì–´ì•¼ í•´.**"
            "í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì•„:\n"
            "{\n"
            "  \"Clothes\": [ {\"name\": \"...\", \"information\": \"...\"}, ... ],\n"
            "  \"ETC\": [ {\"name\": \"...\", \"information\": \"...\"}, ... ]\n"
            "}\n"
            f"\në‚´ìš©:\n{answer}"
        )
        response = gemini_model.generate_content(prompt)
        try:
            cleaned = clean_code_block_json(response.text)
            return json.loads(cleaned)
        except json.JSONDecodeError:
            print("âŒ JSON íŒŒì‹± ì˜¤ë¥˜ (preparation):\n", response.text)
            raise

    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {category}")


# âœ… ì „ì²´ ì²˜ë¦¬
def handle_full_response(question: str) -> dict:
    category = classify_question_with_gemini(question)
    print(f"ğŸ§­ ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬: {category}")
    docs = search_similar_docs(question, category)
    answer = generate_answer_with_gemini(question, docs)
    message = format_message(category, answer)
    summary = generate_summary_with_gemini(answer)
    return {
        "category": category,
        "message": message,
        "summary": summary
    }

# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    question = input("ì—¬í–‰ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    result = handle_full_response(question)
    print("\nğŸ“Œ ì „ì²´ ê²°ê³¼:\n")
    print(json.dumps(result, indent=2, ensure_ascii=False))
