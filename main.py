import os
import faiss
import numpy as np
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from typing import List
from google.generativeai import configure, GenerativeModel

# âœ… .envì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("GEMINI_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# âœ… Gemini API ì„¤ì •
configure(api_key=api_key)
gemini_model = GenerativeModel("models/gemini-1.5-flash")

# âœ… ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# ğŸ”§ ê²½ë¡œ ì„¤ì •
INDEX_ROOT = "embeddings"
DOC_ROOT = "docs"
CATEGORIES = ["contents", "historical", "preparation"]

# ğŸ” ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
def embed_query(text: str) -> np.ndarray:
    embedding = model.encode([text])[0]
    return np.array([embedding], dtype=np.float32)

# ğŸ” ì¹´í…Œê³ ë¦¬ì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
def search_similar_docs(query: str, category: str, top_k: int = 3) -> List[str]:
    index_path = os.path.join(INDEX_ROOT, f"{category}.index")
    path_txt = os.path.join(INDEX_ROOT, f"{category}_paths.txt")

    if not os.path.exists(index_path) or not os.path.exists(path_txt):
        raise FileNotFoundError(f"{category} ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")

    index = faiss.read_index(index_path)
    with open(path_txt, "r", encoding="utf-8") as f:
        id_map = [line.strip() for line in f.readlines()]

    query_vec = embed_query(query)
    distances, indices = index.search(query_vec, top_k)

    results = []
    for idx in indices[0]:
        if idx < len(id_map):
            with open(id_map[idx], "r", encoding="utf-8") as f:
                results.append(f.read().strip())

    return results

# ğŸ”® Gemini APIë¥¼ ì‚¬ìš©í•´ ì§ˆë¬¸ ë¶„ë¥˜
def classify_question_with_gemini(question: str) -> str:
    prompt = (
        "ë‹¤ìŒ ë¬¸ì¥ì´ ì—¬í–‰ì— ëŒ€í•œ ì–´ë–¤ ìœ í˜•ì¸ì§€ ë¶„ë¥˜í•´ ì£¼ì„¸ìš”. "
        "ë§Œì•½, ì§ˆë¬¸ì— ë¬¸í™”, ì—­ì‚¬ì™€ ê°™ì€ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ë©´ historicalë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤."
        "ë§Œì•½, ì§ˆë¬¸ì— ìŒì‹, í•  ê²ƒ, ì¦ê¸¸ê±°ë¦¬, ëª…ì†Œ, í’ê²½ê³¼ ê°™ì€ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ë©´ contentsë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤."
        "ë§Œì•½, ì§ˆë¬¸ì— ì¤€ë¹„í•  ê²ƒ, í•„ìš”í•œ ê²ƒ, ì¤€ë¹„ë¬¼ê³¼ ê°€ì€ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ë©´, preparationë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤."
        "ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”: contents, historical, preparation\n\n"
        f"ë¬¸ì¥: {question}"
    )
    response = gemini_model.generate_content(prompt)
    category = response.text.strip().lower()

    if category not in CATEGORIES:
        raise ValueError(f"ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬ ë°˜í™˜ë¨: {category}")
    return category

# ğŸ’¬ Gemini ì‘ë‹µ ìƒì„±
def generate_answer_with_gemini(question: str, docs: List[str]) -> str:
    context = "\n---\n".join(docs)
    full_prompt = (
        "ë‹¹ì‹ ì€ ì—¬í–‰ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ìœ ìš©í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{question}\n\n"
        f"[ì°¸ê³  ë¬¸ì„œ]\n{context}"
    )
    response = gemini_model.generate_content(full_prompt)
    return response.text.strip()

# ğŸ§  ì „ì²´ íë¦„
def handle_query(question: str) -> str:
    category = classify_question_with_gemini(question)
    print(f"ğŸ§­ ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬: {category}")
    docs = search_similar_docs(question, category)
    answer = generate_answer_with_gemini(question, docs)
    return answer

# ğŸŸ¢ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    question = input("ì—¬í–‰ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    result = handle_query(question)
    print("\nğŸ“Œ ìµœì¢… ë‹µë³€:\n")
    print(result)
